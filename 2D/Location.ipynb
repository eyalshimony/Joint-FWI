{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "839e711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from ctypes import *\n",
    "from scipy.interpolate import LinearNDInterpolator, interp1d\n",
    "from obspy import UTCDateTime\n",
    "import obspy\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.stats import median_abs_deviation\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "import bisect\n",
    "import os\n",
    "import concurrent.futures\n",
    "from itertools import repeat, product\n",
    "from wpca import WPCA\n",
    "from scipy.spatial.distance import cdist, euclidean\n",
    "from skimage.transform import radon, iradon\n",
    "import pygmt\n",
    "import xarray\n",
    "from scipy.signal import find_peaks, lfilter, iirfilter, zpk2sos, sosfiltfilt, savgol_filter\n",
    "import SPECFEM3D_interface\n",
    "import numba\n",
    "import cmcrameri.cm as cmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfda2561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_times(source_x, source_z, vp, vs, x_m, z_m, ind):\n",
    "    so_file = \"/home/moshe/eyal/eikonal/Eikonal3D.so\"\n",
    "    eikonal_3d = CDLL(so_file)\n",
    "    size_x = np.ma.size(vp, axis=0)\n",
    "    size_y = 1\n",
    "    size_z = np.ma.size(vp, axis=1)\n",
    "    vp_flat = np.ndarray.flatten(vp, 'C')\n",
    "    vs_flat = np.ndarray.flatten(vs, 'C')\n",
    "    v_dim = [size_x, size_y, size_z]\n",
    "    d_elem = [x_m/(size_x-1), 250, z_m/(size_z-1)]\n",
    "    source = [int(source_x/(x_m/(size_x-1))), 0, int(source_z/(z_m/(size_z-1)))]\n",
    "    print(source)\n",
    "\n",
    "    eikonal_3d_func = eikonal_3d._Z9eikonal3DPfPiS_S0_S_\n",
    "    eikonal_3d_func.restype = None\n",
    "    vp_flatc = (c_float * len(vp_flat))(*vp_flat)\n",
    "    vs_flatc = (c_float * len(vs_flat))(*vs_flat)\n",
    "    vDim = (c_int * len(v_dim))(*v_dim)\n",
    "    dElem = (c_float * len(d_elem))(*d_elem)\n",
    "    sourceC = (c_int * len(source))(*source)\n",
    "\n",
    "    times_vp = (c_float * len(vp_flat))()\n",
    "    eikonal_3d_func(vp_flatc, vDim, dElem, sourceC, times_vp)\n",
    "    times_vp = np.array(times_vp[:])\n",
    "    times_vp = times_vp.reshape((size_z, size_y, size_x))\n",
    "    times_vp = times_vp.transpose((2, 1, 0))\n",
    "\n",
    "    times_vs = (c_float * len(vs_flat))()\n",
    "    eikonal_3d_func(vs_flatc, vDim, dElem, sourceC, times_vs)\n",
    "    times_vs = np.array(times_vs[:])\n",
    "    times_vs = times_vs.reshape((size_z, size_y, size_x))\n",
    "    times_vs = times_vs.transpose((2, 1, 0))\n",
    "\n",
    "    return ind, times_vp, times_vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "0f53af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity_models(size):\n",
    "    tomographic_models = SPECFEM3D_interface.read_tomographic_models(\"mtinv\", 0)\n",
    "    ind = 0\n",
    "    vp = np.reshape(tomographic_models[:, 2], (size+1, size+1), order='F')\n",
    "    vs = np.reshape(tomographic_models[:, 3], (size+1, size+1), order='F')\n",
    "    return vp[::4, ::4], vs[::4, ::4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "8eccbb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_p, v_s = get_velocity_models(320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89e3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m = 20000\n",
    "z_m = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "b9ef7cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 0, 80]\n",
      "[14, 0, 79]\n",
      "[14, 0, 78]\n",
      "[14, 0, 77]\n",
      "[14, 0, 76]\n",
      "[14, 0, 75]\n",
      "[14, 0, 74]\n",
      "[14, 0, 73]\n",
      "[14, 0, 72]\n",
      "[62, 0, 80]\n",
      "[62, 0, 79]\n",
      "[62, 0, 78]\n",
      "[62, 0, 77]\n",
      "[62, 0, 76]\n",
      "[62, 0, 75]\n",
      "[62, 0, 74]\n",
      "[62, 0, 73]\n",
      "[62, 0, 72]\n",
      "[4, 0, 80]\n",
      "[5, 0, 80]\n",
      "[6, 0, 80]\n",
      "[7, 0, 80]\n",
      "[8, 0, 80]\n",
      "[9, 0, 80]\n",
      "[10, 0, 80]\n",
      "[11, 0, 80]\n",
      "[12, 0, 80]\n",
      "[13, 0, 80]\n",
      "[14, 0, 80]\n",
      "[15, 0, 80]\n",
      "[16, 0, 80]\n",
      "[17, 0, 80]\n",
      "[18, 0, 80]\n",
      "[19, 0, 80]\n",
      "[20, 0, 80]\n",
      "[21, 0, 80]\n",
      "[22, 0, 80]\n",
      "[23, 0, 80]\n",
      "[24, 0, 80]\n",
      "[25, 0, 80]\n",
      "[26, 0, 80]\n",
      "[27, 0, 80]\n",
      "[28, 0, 80]\n",
      "[29, 0, 80]\n",
      "[30, 0, 80]\n",
      "[31, 0, 80]\n",
      "[32, 0, 80]\n",
      "[33, 0, 80]\n",
      "[34, 0, 80]\n",
      "[35, 0, 80]\n",
      "[36, 0, 80]\n",
      "[37, 0, 80]\n",
      "[38, 0, 80]\n",
      "[39, 0, 80]\n",
      "[40, 0, 80]\n",
      "[41, 0, 80]\n",
      "[42, 0, 80]\n",
      "[43, 0, 80]\n",
      "[44, 0, 80]\n",
      "[45, 0, 80]\n",
      "[46, 0, 80]\n",
      "[47, 0, 80]\n",
      "[48, 0, 80]\n",
      "[49, 0, 80]\n",
      "[50, 0, 80]\n",
      "[51, 0, 80]\n",
      "[52, 0, 80]\n",
      "[53, 0, 80]\n",
      "[54, 0, 80]\n",
      "[55, 0, 80]\n",
      "[56, 0, 80]\n",
      "[57, 0, 80]\n",
      "[58, 0, 80]\n",
      "[59, 0, 80]\n",
      "[60, 0, 80]\n",
      "[61, 0, 80]\n",
      "[62, 0, 80]\n",
      "[63, 0, 80]\n",
      "[64, 0, 80]\n",
      "[65, 0, 80]\n",
      "[66, 0, 80]\n",
      "[67, 0, 80]\n",
      "[68, 0, 80]\n",
      "[69, 0, 80]\n",
      "[70, 0, 80]\n",
      "[71, 0, 80]\n",
      "[72, 0, 80]\n",
      "[73, 0, 80]\n",
      "[74, 0, 80]\n",
      "[75, 0, 80]\n",
      "[76, 0, 80]\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "import re\n",
    "\n",
    "stations = data.get_stations()\n",
    "fibres = [\"BI\", \"BII\", \"S\"]\n",
    "\n",
    "stations_eikonal_tables_vp = []\n",
    "stations_eikonal_tables_vs = []\n",
    "\n",
    "for fibre in fibres:\n",
    "    fibre_stations = [station for station in stations if re.sub(r'[^a-zA-Z]', '', station.station) == fibre][::250]\n",
    "    stations_x_s = [station.longitude for station in fibre_stations]\n",
    "    stations_z_s = [20000 - station.burial for station in fibre_stations]\n",
    "    indices = list(range(len(stations_x_s)))\n",
    "    stations_eikonal_tables_vp_s = np.empty((81, 81, len(stations_x_s)))\n",
    "    stations_eikonal_tables_vs_s = np.empty((81, 81, len(stations_x_s)))\n",
    "    for i in range(len(stations_x_s)):\n",
    "        i, eikonal_fibre_vp_curr, eikonal_fibre_vs_curr = calc_times(stations_x_s[i], stations_z_s[i], v_p, v_s, x_m, z_m, \n",
    "                                                                     indices[i])\n",
    "        stations_eikonal_tables_vp_s[:, :, i] = eikonal_fibre_vp_curr[:, 0, :]\n",
    "        stations_eikonal_tables_vs_s[:, :, i] = eikonal_fibre_vs_curr[:, 0, :]\n",
    "    \n",
    "    fibre_stations = [station for station in stations if re.sub(r'[^a-zA-Z]', '', station.station) == fibre][::5]\n",
    "    stations_x = [station.longitude for station in fibre_stations]\n",
    "    stations_z = [20000 - station.burial for station in fibre_stations]\n",
    "    stations_eikonal_tables_vp_curr = np.empty((81, 81, len(stations_x)))\n",
    "    stations_eikonal_tables_vs_curr = np.empty((81, 81, len(stations_x)))\n",
    "    for i in range(stations_eikonal_tables_vp_s.shape[0]):\n",
    "        for j in range(stations_eikonal_tables_vp_s.shape[1]):\n",
    "            if fibre == \"S\":\n",
    "                vp_interp = interp1d(stations_x_s, stations_eikonal_tables_vp_s[i, j, :])\n",
    "                stations_eikonal_tables_vp_curr[i, j, :] = vp_interp(stations_x)\n",
    "                vs_interp = interp1d(stations_x_s, stations_eikonal_tables_vs_s[i, j, :])\n",
    "                stations_eikonal_tables_vs_curr[i, j, :] = vs_interp(stations_x)\n",
    "            else:\n",
    "                vp_interp = interp1d(stations_z_s, stations_eikonal_tables_vp_s[i, j, :])\n",
    "                stations_eikonal_tables_vp_curr[i, j, :] = vp_interp(stations_z)\n",
    "                vs_interp = interp1d(stations_z_s, stations_eikonal_tables_vs_s[i, j, :])\n",
    "                stations_eikonal_tables_vs_curr[i, j, :] = vs_interp(stations_z)\n",
    "    stations_eikonal_tables_vp.append(stations_eikonal_tables_vp_curr)\n",
    "    stations_eikonal_tables_vs.append(stations_eikonal_tables_vs_curr)\n",
    "stations_eikonal_tables_vp = np.concatenate(stations_eikonal_tables_vp, axis=2)\n",
    "stations_eikonal_tables_vs = np.concatenate(stations_eikonal_tables_vs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "412f9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 5\n",
    "min_spat_freq = 0.01 / 2000\n",
    "max_spat_freq = 10 / 2000\n",
    "space_sampling_rate = 1 / dx\n",
    "space_nyquist = space_sampling_rate / 2\n",
    "space_low = min_spat_freq / space_nyquist\n",
    "space_high = max_spat_freq / space_nyquist\n",
    "z_space, p_space, k_space = iirfilter(4, [space_low, space_high], btype='band', ftype='butter', output='zpk')\n",
    "sos_space = zpk2sos(z_space, p_space, k_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "id": "7d778728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_locations(i):\n",
    "    with open(f\"/DATA/eyal/specfem2d/mtinv/observed_seismograms{i:04d}.pk\", \"rb\") as f:\n",
    "        obs1 = pickle.load(f)\n",
    "    obs1.filter(\"lowpass\", freq=10)\n",
    "    waveform_array = np.array([trace.data for trace in obs1])\n",
    "    waveform_array_ext = np.concatenate((np.zeros(waveform_array.shape), waveform_array), axis=1)\n",
    "    waveform_array_ext /= np.max(np.abs(waveform_array_ext), axis=1, keepdims=True)\n",
    "    score, std_score, mad_score = calc_single_slice(np.abs(hilbert(waveform_array_ext, axis=1))[:802,:], dt, dims, \n",
    "                                                    stations_eikonal_tables_vp[:, :, :802], \n",
    "                                                    stations_eikonal_tables_vs[:, :, :802], list(range(401)), \n",
    "                                                    list(range(401, 802)))\n",
    "    comb_score = np.std(mad_score[mad_score < 1000]) / np.std(score[score > 0]) / 4 * score - mad_score\n",
    "    probability = score_to_probability(comb_score)\n",
    "    depth_ind = find_closest_index(np.cumsum(np.sum(probability, axis=0)), 0.5)\n",
    "    if np.sum(probability, axis=0)[depth_ind] / np.sum(probability, axis=0).max() < 0.33:\n",
    "        depth_ind = np.argmax(np.sum(probability, axis=0))\n",
    "    scores_s, mad_scores_s, std_scores_s = calc_scores_reduced(np.abs(hilbert(waveform_array_ext, axis=1)), dt, dims, \n",
    "                                                               stations_eikonal_tables_vp, stations_eikonal_tables_vs, \n",
    "                                                               depth_ind, list(range(401)), list(range(401, 802)), \n",
    "                                                               list(range(802, 4402)))\n",
    "    comb_score_s = np.std(mad_scores_s[mad_scores_s < 1000]) / np.std(scores_s[scores_s > 0]) * scores_s - mad_scores_s\n",
    "    probability_s = score_to_probability(comb_score_s)\n",
    "    spat_ind = find_closest_index(np.cumsum(probability_s), 0.5)\n",
    "    if probability_s[spat_ind] / probability_s.max() < 0.33:\n",
    "        spat_ind = np.argmax(probability_s)\n",
    "    loc = [spat_ind, depth_ind]\n",
    "    return i, score, std_score, mad_score, comb_score, probability, scores_s, mad_scores_s, std_scores_s, comb_score_s, probability_s, loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "id": "58905204",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_all = {}\n",
    "std_score_all = {}\n",
    "mad_score_all = {}\n",
    "comb_score_all = {}\n",
    "probability_all = {}\n",
    "scores_s_all = {}\n",
    "mad_scores_s_all = {}\n",
    "std_scores_s_all = {}\n",
    "comb_score_s_all = {}\n",
    "probability_s_all = {}\n",
    "loc_all = {}\n",
    "inds = list(range(1, 21))\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    for i, score, std_score, mad_score, comb_score, probability, scores_s, mad_scores_s, std_scores_s, comb_score_s, probability_s, loc in executor.map(calc_locations, inds):\n",
    "        score_all[i] = score\n",
    "        std_score_all[i] = std_score\n",
    "        mad_score_all[i] = mad_score\n",
    "        comb_score_all[i] = comb_score\n",
    "        probability_all[i] = probability\n",
    "        scores_s_all[i] = scores_s\n",
    "        mad_scores_s_all[i] = mad_scores_s\n",
    "        std_scores_s_all[i] = std_scores_s\n",
    "        comb_score_s_all[i] = comb_score_s\n",
    "        probability_s_all[i] = probability_s\n",
    "        loc_all[i] = loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "7638796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 21)\n",
    "    with open(\"/DATA/eyal/specfem2d/mtinv/observed_seismograms{i:04d}.pk\", \"rb\") as f:\n",
    "        obs1 = pickle.load(f)\n",
    "    obs1.filter(\"lowpass\", freq=10)\n",
    "    waveform_array = np.array([trace.data for trace in obs1])\n",
    "    waveform_array_ext = np.concatenate((np.zeros(waveform_array.shape), waveform_array), axis=1)\n",
    "    waveform_array_ext /= np.max(np.abs(waveform_array_ext), axis=1, keepdims=True)\n",
    "    score, std_score, mad_score = calc_single_slice(np.abs(hilbert(waveform_array_ext, axis=1))[:802,:], dt, dims, \n",
    "                                                    stations_eikonal_tables_vp[:, :, :802], \n",
    "                                                    stations_eikonal_tables_vs[:, :, :802], list(range(401)), \n",
    "                                                    list(range(401, 802)))\n",
    "    comb_score = np.std(mad_score[mad_score < 1000]) / np.std(score[score > 0]) / 4 * score - mad_score\n",
    "    probability = score_to_probability(comb_score)\n",
    "    depth_ind = find_closest_index(np.cumsum(np.sum(probability, axis=0)), 0.5)\n",
    "    scores_s, mad_scores_s, std_scores_s = calc_scores_reduced(np.abs(hilbert(waveform_array_ext, axis=1)), dt, dims, \n",
    "                                                               stations_eikonal_tables_vp, stations_eikonal_tables_vs, \n",
    "                                                               depth_ind, list(range(401)), list(range(401, 802)), \n",
    "                                                               list(range(802, 4402)))\n",
    "    comb_score_s = np.std(mad_scores_s[mad_scores_s < 1000]) / np.std(scores_s[scores_s > 0]) * scores_s - mad_scores_s\n",
    "    probability_s = score_to_probability(comb_score_s)\n",
    "    loc = [probability_s.argmax(), depth_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "732ede9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_single_shifted_waveform_clean(waveform_array, dt, stations_eikonal_tables_vp, stations_eikonal_tables_vs):\n",
    "    shifted_waveform_array_p = np.zeros(waveform_array.shape)\n",
    "    shifted_waveform_array_s = np.zeros(waveform_array.shape)\n",
    "\n",
    "    for l in range(np.ma.size(waveform_array, axis=0)):\n",
    "        shifted_waveform_array_p[l, :] = \\\n",
    "            np.roll(waveform_array[l, :],\n",
    "                    -int(np.round(stations_eikonal_tables_vp[l] / dt)))\n",
    "        shifted_waveform_array_s[l, :] = \\\n",
    "            np.roll(waveform_array[l, :],\n",
    "                    -int(np.round(stations_eikonal_tables_vs[l] / dt)))\n",
    "\n",
    "    return shifted_waveform_array_p, shifted_waveform_array_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bdafc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s_index(subarray_sum):\n",
    "    s_sus_p_max = int(np.where(subarray_sum > subarray_sum.max()/5)[0][0] + 0.5/dt)\n",
    "    if s_sus_p_max >= len(subarray_sum):\n",
    "        return subarray_sum.argmax()\n",
    "    s_sus_s_max = subarray_sum[s_sus_p_max:].max()\n",
    "    s_sus_s_argmax = subarray_sum[s_sus_p_max:].argmax() + s_sus_p_max\n",
    "    if s_sus_p_max == s_sus_s_argmax:\n",
    "        return subarray_sum.argmax()\n",
    "    if s_sus_s_max / subarray_sum.max() > 0.5 and subarray_sum[s_sus_p_max:s_sus_s_argmax].min() / s_sus_s_max < 0.5:\n",
    "        return s_sus_s_argmax\n",
    "    else:\n",
    "        return subarray_sum.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d3be70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit(fastmath=True, cache=True)\n",
    "def shift_data(waveform_array, stations_eikonal_tables_vp, stations_eikonal_tables_vs, inv_dt, num_stations, num_samples):\n",
    "    shifted_waveform_array_p = np.empty((num_stations, num_samples), dtype=waveform_array.dtype)\n",
    "    shifted_waveform_array_s = np.empty((num_stations, num_samples), dtype=waveform_array.dtype)\n",
    "\n",
    "    for l in range(num_stations):\n",
    "        # Calculate integer shifts for this station\n",
    "        shift_p = -int(np.round(stations_eikonal_tables_vp[l] * inv_dt))\n",
    "        shift_s = -int(np.round(stations_eikonal_tables_vs[l] * inv_dt))\n",
    "\n",
    "        # Apply roll manually using modulo indexing\n",
    "        for sample_idx in range(num_samples):\n",
    "            # Calculate source index based on the desired shift\n",
    "            src_idx_p = (sample_idx - shift_p) % num_samples\n",
    "            src_idx_s = (sample_idx - shift_s) % num_samples\n",
    "            # Assign value from source index to current target index\n",
    "            shifted_waveform_array_p[l, sample_idx] = waveform_array[l, src_idx_p]\n",
    "            shifted_waveform_array_s[l, sample_idx] = waveform_array[l, src_idx_s]\n",
    "    return shifted_waveform_array_p, shifted_waveform_array_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "98f33a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_single_slice(waveform_array, dt, dims, stations_eikonal_tables_vp, stations_eikonal_tables_vs, inds_a1, inds_a2):\n",
    "    score = np.zeros((dims[0], dims[1]))\n",
    "    std_score = np.zeros((dims[0], dims[1]))\n",
    "    mad_score = np.zeros((dims[0], dims[1]))\n",
    "    num_stations = np.ma.size(waveform_array, axis=0)\n",
    "    num_samples = np.ma.size(waveform_array, axis=1)\n",
    "    waveform_array /= np.max(np.abs(waveform_array), axis=1, keepdims=True)\n",
    "\n",
    "    for i in range(dims[0]):\n",
    "        for j in range(dims[1]):\n",
    "            shifted_waveform_array_p, shifted_waveform_array_s = shift_data(waveform_array, stations_eikonal_tables_vp[i, j, :], \n",
    "                                                                            stations_eikonal_tables_vs[i, j, :], 1/dt, num_stations, num_samples)\n",
    "\n",
    "            shifted_waveform_array_p_a1 = shifted_waveform_array_p[inds_a1]\n",
    "            shifted_waveform_array_s_a1 = shifted_waveform_array_s[inds_a1]\n",
    "            shifted_waveform_array_p_a2 = shifted_waveform_array_p[inds_a2]\n",
    "            shifted_waveform_array_s_a2 = shifted_waveform_array_s[inds_a2]\n",
    "            \n",
    "            p_a1_sum = savgol_filter(np.abs(shifted_waveform_array_p_a1.sum(axis=0) / len(inds_a1)), int(0.3/dt), 4)\n",
    "            s_a1_sum = savgol_filter(np.abs(shifted_waveform_array_s_a1.sum(axis=0) / len(inds_a1)), int(0.3/dt), 4)\n",
    "            p_a2_sum = savgol_filter(np.abs(shifted_waveform_array_p_a2.sum(axis=0) / len(inds_a2)), int(0.3/dt), 4)\n",
    "            s_a2_sum = savgol_filter(np.abs(shifted_waveform_array_s_a2.sum(axis=0) / len(inds_a2)), int(0.3/dt), 4)\n",
    "\n",
    "            innds_pa1, _ = find_peaks(p_a1_sum, height=0.11, distance=10)\n",
    "            innds_sa1, _ = find_peaks(s_a1_sum, height=0.5, distance=10)\n",
    "            innds_pa2, _ = find_peaks(p_a2_sum, height=0.11, distance=10)\n",
    "            innds_sa2, _ = find_peaks(s_a2_sum, height=0.5, distance=10)\n",
    "\n",
    "            \n",
    "            if len(innds_pa1) == 0 or len(innds_sa1) == 0 or len(innds_pa2) == 0 or len(innds_sa2) == 0:\n",
    "                score[i, j] = 0\n",
    "                mad_score[i, j] = 1000\n",
    "                std_score[i, j] = 1000\n",
    "                continue\n",
    "                \n",
    "            p_a1_max_ind = innds_pa1[0]\n",
    "            max_peak = 0\n",
    "            for peak in innds_sa1:\n",
    "                if s_a1_sum[peak] > max_peak:\n",
    "                    s_a1_max_ind = peak\n",
    "            p_a2_max_ind = innds_pa2[0]\n",
    "            for peak in innds_sa2:\n",
    "                if s_a2_sum[peak] > max_peak:\n",
    "                    s_a2_max_ind = peak\n",
    "\n",
    "            p_a1_max = p_a1_sum[p_a1_max_ind]\n",
    "            s_a1_max = s_a1_sum[s_a1_max_ind]\n",
    "            p_a2_max = p_a2_sum[p_a2_max_ind]\n",
    "            s_a2_max = s_a2_sum[s_a2_max_ind]\n",
    "            \n",
    "            score[i, j] = p_a1_max + s_a1_max + p_a2_max + s_a2_max\n",
    "\n",
    "            mad_score[i, j] = median_abs_deviation([p_a1_max_ind, s_a1_max_ind, p_a2_max_ind, s_a2_max_ind])\n",
    "\n",
    "            std_score[i, j] = np.std([p_a1_max_ind, s_a1_max_ind, p_a2_max_ind, s_a2_max_ind])\n",
    "            \n",
    "    return score, std_score, mad_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "id": "bd083ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores_reduced(waveform_array, dt, dims, stations_eikonal_tables_vp, stations_eikonal_tables_vs, j, inds_a1, inds_a2, inds_a3):\n",
    "    score = np.zeros((dims[0]))\n",
    "    mad_score = np.zeros((dims[0]))\n",
    "    std_score = np.zeros((dims[0]))\n",
    "    num_stations = np.ma.size(waveform_array, axis=0)\n",
    "    num_samples = np.ma.size(waveform_array, axis=1)\n",
    "    waveform_array /= np.max(np.abs(waveform_array), axis=1, keepdims=True)\n",
    "\n",
    "    for i in range(dims[0]):\n",
    "        shifted_waveform_array_p, shifted_waveform_array_s = shift_data(waveform_array, stations_eikonal_tables_vp[i, j, :], \n",
    "                                                                        stations_eikonal_tables_vs[i, j, :], 1/dt, num_stations, num_samples)\n",
    "        \n",
    "        shifted_waveform_array_p_a1 = shifted_waveform_array_p[inds_a1]\n",
    "        shifted_waveform_array_s_a1 = shifted_waveform_array_s[inds_a1]\n",
    "        shifted_waveform_array_p_a2 = shifted_waveform_array_p[inds_a2]\n",
    "        shifted_waveform_array_s_a2 = shifted_waveform_array_s[inds_a2]\n",
    "        shifted_waveform_array_s_a3 = shifted_waveform_array_s[inds_a3]\n",
    "\n",
    "        p_a1_sum = np.abs(shifted_waveform_array_p_a1.sum(axis=0)) / (len(inds_a1) + len(inds_a2) + len(inds_a3))\n",
    "        s_a1_sum = np.abs(shifted_waveform_array_s_a1.sum(axis=0)) / (len(inds_a1) + len(inds_a2) + len(inds_a3))\n",
    "        p_a2_sum = np.abs(shifted_waveform_array_p_a2.sum(axis=0)) / (len(inds_a1) + len(inds_a2) + len(inds_a3))\n",
    "        s_a2_sum = np.abs(shifted_waveform_array_s_a2.sum(axis=0)) / (len(inds_a1) + len(inds_a2) + len(inds_a3))\n",
    "        s_a3_sum = np.abs(shifted_waveform_array_s_a3.sum(axis=0)) / (len(inds_a1) + len(inds_a2) + len(inds_a3))\n",
    "\n",
    "        innds_pa1, _ = find_peaks(p_a1_sum, height=0.11 * len(inds_a1) / (len(inds_a1) + len(inds_a2) + len(inds_a3)))\n",
    "        innds_sa1, _ = find_peaks(s_a1_sum, height=0.5 * len(inds_a1) / (len(inds_a1) + len(inds_a2) + len(inds_a3)))\n",
    "        innds_pa2, _ = find_peaks(p_a2_sum, height=0.11 * len(inds_a2) / (len(inds_a1) + len(inds_a2) + len(inds_a3)))\n",
    "        innds_sa2, _ = find_peaks(s_a2_sum, height=0.5 * len(inds_a2) / (len(inds_a1) + len(inds_a2) + len(inds_a3)))\n",
    "        innds_sa3, _ = find_peaks(s_a3_sum, height=0.5 * len(inds_a3) / (len(inds_a1) + len(inds_a2) + len(inds_a3)))\n",
    "\n",
    "        if len(innds_pa1) == 0 or len(innds_sa1) == 0 or len(innds_pa2) == 0 or len(innds_sa2) == 0 or len(innds_sa3) == 0:\n",
    "            score[i] = 0\n",
    "            mad_score[i] = 1000\n",
    "            std_score[i] = 1000\n",
    "            continue\n",
    "\n",
    "        p_a1_max_ind = innds_pa1[0]\n",
    "        s_a1_max_ind = innds_sa1[0]\n",
    "        p_a2_max_ind = innds_pa2[0]\n",
    "        s_a2_max_ind = innds_sa2[0]\n",
    "        s_a3_max_ind = innds_sa3[0]\n",
    "\n",
    "        p_a1_max = p_a1_sum[p_a1_max_ind]\n",
    "        s_a1_max = s_a1_sum[s_a1_max_ind]\n",
    "        p_a2_max = p_a2_sum[p_a2_max_ind]\n",
    "        s_a2_max = s_a2_sum[s_a2_max_ind]\n",
    "        s_a3_max = s_a3_sum[s_a3_max_ind]\n",
    "\n",
    "        score[i] = p_a1_max + s_a1_max + p_a2_max + s_a2_max + s_a3_max\n",
    "\n",
    "        mad_score[i] = median_abs_deviation([p_a1_max_ind, s_a1_max_ind, p_a2_max_ind, s_a2_max_ind, s_a3_max_ind])\n",
    "\n",
    "        std_score[i] = np.std([p_a1_max_ind, s_a1_max_ind, p_a2_max_ind, s_a2_max_ind, s_a3_max_ind])\n",
    "            \n",
    "    return score, mad_score, std_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "bccb19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_index(arr: np.ndarray, value: float) -> int:\n",
    "    \"\"\"\n",
    "    Finds the index of the element in a sorted NumPy array that is closest\n",
    "    in value to the requested 'value'.\n",
    "\n",
    "    Args:\n",
    "        arr (np.ndarray): A 1D NumPy array that must be sorted in ascending order.\n",
    "        value (float): The value for which to find the closest element's index.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the element in 'arr' that is closest to 'value'.\n",
    "             In case of a tie (value is equidistant from two elements),\n",
    "             it defaults to the larger element's index.\n",
    "    \"\"\"\n",
    "    if not np.all(arr[:-1] <= arr[1:]): # Basic check for sorted array\n",
    "        raise ValueError(\"Input array must be sorted in ascending order.\")\n",
    "\n",
    "    # Find the index where 'value' would be inserted to maintain sorted order.\n",
    "    # 'side='left'' means it returns the first index 'i' such that arr[i] >= value.\n",
    "    idx = np.searchsorted(arr, value, side='left')\n",
    "\n",
    "    # Handle edge cases where 'value' is outside the array's range\n",
    "    if idx == 0:\n",
    "        # 'value' is less than or equal to the first element, so the first element is closest.\n",
    "        return 0\n",
    "    elif idx == len(arr):\n",
    "        # 'value' is greater than the last element, so the last element is closest.\n",
    "        return len(arr) - 1\n",
    "    else:\n",
    "        # 'value' falls between arr[idx-1] and arr[idx]\n",
    "        # Compare the absolute difference to find which is closer\n",
    "        diff_left = abs(value - arr[idx-1])\n",
    "        diff_right = abs(value - arr[idx])\n",
    "\n",
    "        if diff_left < diff_right:\n",
    "            return idx - 1  # arr[idx-1] is closer\n",
    "        else:\n",
    "            return idx      # arr[idx] is closer or equidistant (defaults to right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "09e6a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_probability(scores_array):\n",
    "    prob_array = scores_array.copy()\n",
    "    prob_array -= prob_array[prob_array>-1000].min()\n",
    "    wanted_half_val = prob_array.max() / 40\n",
    "    prob_array -= prob_array.max()\n",
    "    prob_array /= (wanted_half_val / np.log(2 * np.e - 1))\n",
    "    prob_array[prob_array < -300] = -300\n",
    "    prob_array = 1 / (1 + np.exp(-prob_array))\n",
    "    prob_array /= prob_array.sum()\n",
    "\n",
    "    return prob_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "5ff9e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_geometric_median(probability, eps=1e-5):\n",
    "    points, weights = probability_to_point_list_prob(probability)\n",
    "    y = np.mean(points, 0)\n",
    "\n",
    "    while True:\n",
    "        D = cdist(points, [y])\n",
    "        nonzeros = (D != 0)[:, 0]\n",
    "        zeros = (D == 0)[:, 0]\n",
    "        Dinv = weights[nonzeros] / D[nonzeros]\n",
    "        Dinvs = np.sum(Dinv)\n",
    "        W = Dinv / Dinvs\n",
    "        T = np.sum(W * points[nonzeros], 0)\n",
    "        num_zeros = len(points) - np.sum(nonzeros)\n",
    "        if num_zeros == 0:\n",
    "            y1 = T\n",
    "        elif num_zeros == len(points):\n",
    "            return y\n",
    "        else:\n",
    "            R = (T - y) * Dinvs\n",
    "            r = np.linalg.norm(R)\n",
    "            eta = 0 if num_zeros == 0 else weights[zeros][0]\n",
    "            rinv = 0 if r == 0 else eta / r\n",
    "            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n",
    "        if euclidean(y, y1) < eps:\n",
    "            return y1\n",
    "        y = y1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "30b7b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_point_list_prob(probability):\n",
    "    xs = list(range(np.ma.size(probability, axis=0)))\n",
    "    ys = list(range(np.ma.size(probability, axis=1)))\n",
    "    zs = list(range(np.ma.size(probability, axis=2)))\n",
    "\n",
    "    point_list = np.asarray(list(product(xs, ys, zs)))\n",
    "    probability_1d = probability.ravel()\n",
    "    probability_1d = probability_1d.reshape((np.ma.size(point_list, axis=0), 1))\n",
    "\n",
    "    return point_list, probability_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "f9bc8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_confidence_ellipsoid(probability):\n",
    "    points, weights = probability_to_point_list_prob(probability)\n",
    "    weights_ext = np.squeeze(np.dstack([weights, weights, weights]))\n",
    "\n",
    "    pca = WPCA().fit(points, weights=weights_ext)\n",
    "    ellipsoid_orientation = pca.components_\n",
    "    pca.explained_variance_[pca.explained_variance_ < 0] = 0\n",
    "    half_uncertainties = np.sqrt(7.815 * pca.explained_variance_)\n",
    "\n",
    "    return half_uncertainties, ellipsoid_orientation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
